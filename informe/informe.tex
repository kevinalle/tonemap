\documentclass[a4paper,10pt]{article}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}

\usepackage{color}
\usepackage{amsmath}

\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\vect}[1]{\boldsymbol{\vec{#1}}}
\newcommand{\versor}[1]{\boldsymbol{\hat{#1}}}
\newcommand{\attention}[1]{ {\color{red}#1} }
\newcommand{\revisar}[1]{\textcolor{yellow}{#1}}

\title{Una implementación del algoritmo Exposure Fusion bajo el paradigma SIMD}
\author{Kevin Allekotte, Thomas Fischer}
\date{\today}

\begin{document}

    \maketitle

    \begin{abstract}

    \end{abstract}

    \section{Introducción}

        \attention{Rango de una cámara. Cortar del apéndice}

        El rango dinámico del ojo humano es dificil de calcular, pero bajo condiciones parecidas a las de un sistema de fotografía, está estimado entre $10$ a $14 f-stops$. Vemos la gran diferencia que hay entre un sensor digital y el ojo humano, por lo que reproducir adecuadamente lo que ve un ojo es imposible bajo condiciones de mucho rango dinámico, considerando que un día soleado con reflectividad dispareja tiene un rango de $12+ f-stops$.

        Las imágenes HDR (High Dynamic Range) son imágenes que tienen un rango dinámico mucho más grande, y por lo tanto resultan más representativas y mas detalladas que las imágenes comunes. HDRI (HDR Imaging) son métodos utilizados para crear imágenes HDR combinando la información de varias imágenes de la misma escena con menor rango dinámico bajo distintas exposiciones. \attention{problemas del HDR}

        Exposure Fusion \cite{DBLP:conf/pg/MertensKR07} es una técnica desarrollada para fusionar varias imágenes de bajo rango dinámico tomadas a distintas exposiciones  con un resultado similar a los operadores de tone mapping de HDR. Esta técnica evita pasos como la creación de la imágen HDR, la calibración de la curva de respuesta de la cámara y la selección de operadores de tone mapping, pasos que son costosos computacionalmente e implican una calibración "a ojo" de los parámetros.
        
        Exposure fusion analiza ciertas propiedades de las imágenes que son relevantes para la calidad de la foto, y por cada pixel de la imágen se queda con la "mejor" información que puede recuperar a partir de todas las imágenes originales. Estas propiedades son el contraste, la saturación y la calidad de exposición de un pixel. El resultado final es computado automáticamente.

    \section{Algoritmo}

        Cada pixel de la imágen final se obtiene como la suma ponderada de los valores de ese pixel en las imágenes de entrada. El peso que se le asigna a cada muestra es la medida de la calidad de ese pixel en esa imágen. En resumen:

        $$ R_{i,j} = \sum_{k=1}^N{ \hat{W}_{k_{i,j}}*I_{k_{i,j}} } $$

        Donde $N$ es la cantidad de imágenes a fusionar, $I_k$ son esas imágenes, $\hat{W}_{k_{i,j}}$ es el peso del pixels $I_{k_{i,j}}$ y $R_{i,j}$ es la imágen resultante.

        Para calcular los pesos $\hat{W}_{k,i,j}$, primero calculamos la calidad de cada pixel en función de las tres propiedades descriptas anteriormente

        $$ W_{k_{i,j}} = (C_{k_{i,j}})^{w_C} \times (S_{k_{i,j}})^{w_S} \times (E_{k_{i,j}})^{w_E} $$

        donde $w_C$, $w_S$ y $w_E$ son parámetros que regulan cuanta importancia le queremos dar a cada métrica, y luego normalizamos los pesos de las matrices $W_{k_{i,j}}$ para que el resultado sume 1 en cada pixel, y el resultado tenga el mismo \attention{depth bit} que las imágenes originales.

        $$ \hat{W}_{k_{i,j}} = W_{k_{i,j}} \times [\sum_{k'=1}^N{W_{k'_{i,j}}}]^{-1} $$ 

        Nos falta entonces calcular las matrices de calidad respectivas a cada propiedad.

        \subsection{Contraste}

            Aplicamos un filtro laplaciano \attention{REF! apendice o algo} a la versión en escala de grises de cada imágen y nos quedamos con el valor absoluto de la respuesta. Esto suele asignar pesos altos a elementos que pertenecen a bordes o texturas en la imágen.

        \subsection{Saturación}

            A medida que se toman imágen con cada vez más exposición, los colores tienden a dessaturarse. Los colores saturados son los que más nos interesan si queremos una imágen con colores vívidos ... \attention{explicar mejor que es saturacion}

        \subsection{Exposición}

            Una pixel bien expuesto es aquel que no tiene una intensidad cercana al mínimo (\attention{bajosaturada?}) o al máximo (sobresaturada) del rango dinámico. Luego, definimos la calidad de la exposición como la distancia al valor óptimo $0.5$, siguiendo una curva gaussiana $exp(-\frac{(i-0.5)^2}{2\sigma^2})$, donde proponemos $\sigma = 0.2$ como en \cite{DBLP:conf/pg/MertensKR07}. En imágenes de color aplicamos esta función por cada canal y multiplicamos los resultados.

    \section{Implementación}

    \section{Resultados}

    \section{Análisis de performance}

    \section{Conclusiones}

    \appendix

    \section{Rango dinámico}

        El rango dinámico en una cámara de fotos, se define como la razón entre el nivel de blanco más alto que puede detectar un sensor y el nivel más bajo. Los sensores de cámaras digitales en blanco y negro, consisten de una grilla de fotositos, que se pueden ver como un recipiente con capacidad para cierta cantidad de fotones. Luego de la exposición, se mide la cantidad de fotones que se encuentran en cada fotosito, y eso nos da el valor lumínico de cada pixel. Si nuestro recipiente se llena, decimos que el pixel esta saturado y va a detectar como valores iguales a todos los que esten por encima del máximo, luego, estamos perdiendo detalle en las zonas claras. Si disminuímos la exposición de la imágen para contrarrestar la saturación, estamos dividiendo la cantidad de fotones que inciden en un recipiente por algun factor, con lo cuál los valores que se encuentran en las zonas oscuras son cada vez más parecidos, y son mas indistinguibles del ruido electrónico generado al leer el valor, por lo que perdemos detalle en las zonas oscuras. Claramente el rango dinámico depende del tamaño de estos recipientes, por ejemplo las cámaras compactas, comparadas por ejemplo con SLR's profesionales, al ser mas chicas deben reducir el tamaño del sensor y por lo tanto el de los fotositos, disminuyendo así el rango dinámico. El rango dinámicos se mide con distintas unidades, de las cuáles la mas común es el f-stop, que describe el rango en potencias de 2, por ejemplo el rango $1024:1 = 1024 = 2^{10} f-stops$.
        En imágenes a color, simplemente tenemos 4 fotositos por pixel, donde cada uno detecta análogamente solamente en un rango de frecuencias cerca de rojo, verde o azul. 

    \bibliographystyle{plain}
    \bibliography{all}

\end{document}
